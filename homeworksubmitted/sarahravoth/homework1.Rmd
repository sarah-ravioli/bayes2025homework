---
title: "homework1"
author: "sarah ravoth"
date: "2025-01-17"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---
```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(rstanarm)
library(tidyverse)
```


**1 -- Simulate data for a linear regression problem (continuous
predictor and response variables) of your choice and show a linear model
can return the parameters you set. Your n should be 100 and you should
set your error (sigma) to a level you would expect in an ecological or
evolutionary biology study.**

How does (larval) predatory mosquito body size change with habitat size
(i.e. water holding capacity of host plant)? n = sample size y = body
size (mm) x = habitat size (litres) a = alpha/intercept b = beta/slope

yi = a + b\*xi + error error \~ normal(0, sigma)

```{r, set.seed(777)}

# set parameters
n = 100
a = 0.5
b = 0.3
sigma <- 0.5
x1 = rnorm(n=n, mean=4, sd=1)
hist(x1) # check
error <- rnorm(n, mean=0, sd=sigma)
hist(error) # check

# generate y values 
ypred <- a + b*x1
yobs <- ypred + error

# check if we can get model parameters back 
lm1 <- stan_glm(yobs ~ x1)
summary(lm1)
coef(lm1) # estimate ok, not super precise but ballpark

# check visually
plot(yobs ~ x1, xlim=c(0, 8))
abline(a=coef(lm1)[1], b=coef(lm1)[2], col="salmon")
```

**Next, add a binary covariate (for example, a treatment and control) to
your dataset that interacts with your other predictor variable. Make the
effect size of this interaction HALF the effect size of the first
predictor variable, and again test how well you can return the parameter
values.**

binary covariate = x2 = closed (0) or open (1) tree canopy The effect
size of open canopy = 0.7, since predatory mosquitoes do better (survive +
grow) in open habitats, where they escape predation pressure of
damselfly larvae, which are more common in forests.

```{r, set.seed(7777)}
# yi = a + b1*x1 + b2*x2 + b3*x1*x2

# set parameters
n = 100
a = 0.5 # same as above 
b1 = 0.3
b2 = 0.7
b3 = 0.15
sigma = 0.5
# x1 = rnorm(n=n, mean=4, sd=1) # same as above
hist(x1) # check
x2 = rbinom(n, size=1, prob=0.5)
hist(x2) # check
# error <- rnorm(n, mean=0, sd=sigma) # same as above
hist(error) # check

# generate y values 
ypred <- a + b1*x1 + b2*x2 + b3*x1*x2
yobs <- ypred + error

# check if we can get model parameters back 
lm2 <- stan_glm(yobs ~ x1 + x2 + x1*x2)
summary(lm2)
coef(lm2) # pretty ok, except intercept worst estimated

# check visually
data.frame(yobs, x1, x2) %>% 
  mutate(x2 = as.character(x2)) %>% 
  ggplot(aes(x=x1, y=yobs, col=x2)) +
  geom_point() + 
  theme_bw() +
  geom_abline(intercept=coef(lm2)[1], slope=coef(lm2)[2], col="salmon") + 
  geom_abline(intercept=coef(lm2)[3], slope=coef(lm2)[2]+coef(lm2)[4], col="turquoise") 

```

**Step 2 -- Building on the above example, let's look at how the
estimates of parameters returned from your linear model change with
sample size, and make our analyses a little more robust.

Using just 20% of your data, see how well you can return the parameters
you set.**

original parameters
a = 0.5 
b1 = 0.3
b2 = 0.7
b3 = 0.15
sigma = 0.5

```{r}
x1_sub <- sample(x1, size=20, replace=FALSE)
x2_sub <- rbinom(n=20, size=1, prob=0.5)
error_sub <- sample(error, size=20, replace=FALSE)
hist(x1_sub) # check
hist(x2_sub) # check
hist(error_sub) # check

# generate y values 
ypred_sub <- a + b1*x1_sub + b2*x2_sub + b3*x1_sub*x2_sub
yobs_sub <- ypred_sub + error_sub

# check if we can get model parameters back 
lm3 <- stan_glm(yobs_sub ~ x1_sub + x2_sub + x1_sub*x2_sub)
summary(lm3)
coef(lm3) 

# ....worst at estimating x2 & itxn term, x1 best
```


**Next, let's make a plot of sampling from your data from 1 to the n of
your data (100) showing the estimated parameters for each sample size.
Make a plot with your 1:100 on the horizontal axis and your estimated
parameters on the vertical (you need either as many plots as parameters
or a way to show all the different parameters on one plot). Compare how
well the model does across the different parameters. Which is better or
worse at and why?**

For all parameters, as sample size increases, the parameter value is estimated more accurately. Across the different sample sizes, the slope type parameters (x1, x1x2_intxn) are better estimated than intercept type parameters (intercept, x2).

```{r, results="hide", warning=FALSE}
# note: chunk results not shown to save space from for loop printing out a bunch of stan_glm messages

sample_sizes <- seq(5,100,5) # sample from small to large sample size (5-100) in increments of 5 
n=100 # set max sample size 
loop_coefs <- data.frame() # make df to save results

for(i in 1:length(sample_sizes)){
  
  # from original parameters, sample i values 
  x1_sub <- sample(x1, size=sample_sizes[i], replace=FALSE)
  x2_sub <- rbinom(n=sample_sizes[i], size=1, prob=0.5)
  error_sub <- sample(error, size=sample_sizes[i], replace=FALSE)

  # generate y values 
  ypred_sub <- a + b1*x1_sub + b2*x2_sub + b3*x1_sub*x2_sub
  yobs_sub <- ypred_sub + error_sub
  
  # check if we can get model parameters back 
  lm_out <- stan_glm(yobs_sub ~ x1_sub + x2_sub + x1_sub*x2_sub)
  loop_coefs <- rbind(loop_coefs, cbind(sample_sizes[i], t(as.data.frame(coef(lm_out)))))
  print(i)
}

# check
head(loop_coefs) 
tail(loop_coefs) 
glimpse(loop_coefs)
```

```{r}
# make a df with the true values
truth <- data.frame(intercept=a, x1=0.3, x2=1, x1x2_intxn=0.15) %>% 
   pivot_longer(intercept:x1x2_intxn, names_to = "parameter", values_to = "value")

# colours represent various parameter esimates are various sample sizes, black dots = the true parameter value
loop_coefs %>% 
  rename(intercept = `(Intercept)`, 
         x1 = x1_sub, 
         x2 = x2_sub, 
         x1x2_intxn = `x1_sub:x2_sub`, 
         sample_size = V1) %>% 
  pivot_longer(intercept:x1x2_intxn, names_to = "parameter", values_to = "value") %>% 
  ggplot(aes(x=parameter, y=value, col=sample_size)) + 
  geom_jitter(width=0.15, size = 2, alpha=0.6) +
  theme_bw() +
  geom_point(aes(x=parameter, y=value), data=truth, col="black", cex=3.5) +
  scale_y_continuous(limits=c(-4, 4), breaks=seq(-4,4,1)) # set y axis to facilitate comparison across diff plots

```

Let's improve our sampling now. So far we have take just ONE draw from
our set of parameters which means our sigma term has some Monte Carlo
error in it, so let's take 10 draws each time (so we need to set up a
loop or such in R that samples from 1:n and at each step it repeats
simulating the data and getting the estimates 10 times). Re-make your
plot. (If you get stuck here for a while, don't panic, but move onto
Steps 3-4.)

```{r, results="hide"}
# note: chunk results not shown to save space from for loop printing out 99 stan_glm messages
 
n=rep(seq(10,100,5), each=10) # set max sample size 
loop_coefs2 <- data.frame() # make df to save results

for(i in 1:length(n)){
  
  # from original parameters, sample i values 
  x1_sub <- sample(x1, size=n[i], replace=FALSE)
  x2_sub <- rbinom(n=n[i], size=1, prob=0.5)
  error_sub <- sample(error, size=n[i], replace=FALSE)

  # generate y values 
  ypred_sub <- a + b1*x1_sub + b2*x2_sub + b3*x1_sub*x2_sub
  yobs_sub <- ypred_sub + error_sub
  
  # check if we can get model parameters back 
  lm_out <- stan_glm(yobs_sub ~ x1_sub + x2_sub + x1_sub*x2_sub)
  loop_coefs2 <- rbind(loop_coefs2, cbind(n[i], t(as.data.frame(coef(lm_out)))))
  print(i)

}

# check
head(loop_coefs2) 
tail(loop_coefs2) 
glimpse(loop_coefs2)
```


```{r}
# make a df with the true values
truth <- data.frame(intercept=a, x1=0.3, x2=1, x1x2_intxn=0.15) %>% 
   pivot_longer(intercept:x1x2_intxn, names_to = "parameter", values_to = "value")

loop_coefs2 %>% 
  rename(intercept = `(Intercept)`, 
         x1 = x1_sub, 
         x2 = x2_sub, 
         x1x2_intxn = `x1_sub:x2_sub`, 
         sample_size = V1) %>% 
  pivot_longer(intercept:x1x2_intxn, names_to = "parameter", values_to = "value") %>% 
  ggplot(aes(x=parameter, y=value, col=sample_size)) + 
  geom_jitter(width=0.15, size = 2, alpha=0.6) +
  theme_bw() +
  geom_point(aes(x=parameter, y=value), data=truth, col="black", cex=3.5) +
  scale_y_continuous(limits=c(-4, 4), breaks=seq(-4,4,1)) # set y axis to facilitate comparison across diff plots
```

**Step 3 -- Now, repeat the above but change your error term by 50%.
Compare these results to what you found before and explain why you think
they changed.**

i decreased the error by 50% (sigma=0.5 to sigma=0.25), and this resulted in more precise estimates of all parameters across all sample sizes, although the same parameters (x1, x1x2_intxn) were still better estimated overall, since there was less error, or randomly generated noise in the simulated data. probably the "peaks" in parameter space were narrower and taller.

```{r, results="hide", warning=FALSE}
# note: chunk results not shown to save space from for loop printing out 99 stan_glm messages

# reduce error term by 50% 
sigma2 <- 0.25
error2 <- rnorm(n, mean=0, sd=sigma2)

n=rep(seq(5,100,5), each=10) # set max sample size 
loop_coefs3 <- data.frame() # make df to save results
loop_ppd <- data.frame()
for(i in 1:length(n)){
  
  # from original parameters, sample i values 
  x1_sub <- sample(x1, size=n[i], replace=FALSE)
  x2_sub <- rbinom(n=n[i], size=1, prob=0.5)
  error_sub <- sample(error2, size=n[i], replace=FALSE)

  # generate y values 
  ypred_sub <- a + b1*x1_sub + b2*x2_sub + b3*x1_sub*x2_sub
  yobs_sub <- ypred_sub + error_sub
  
  # check if we can get model parameters back 
  lm_out <- stan_glm(yobs_sub ~ x1_sub + x2_sub + x1_sub*x2_sub)
  
  # save outputs
  loop_coefs3 <- rbind(loop_coefs3, cbind(n[i], t(as.data.frame(coef(lm_out))))) # for parameters
  ppd_est <- summary(lm_out) %>% 
    as.data.frame() %>% 
    dplyr::select(mean, `10%`, `50%`, `90%`) %>%   # extract mean & uncertainty intervals
    rownames_to_column(var="parameter") %>%    # make rownames a col
    filter(parameter=="mean_PPD") %>%   # select just ppd
    mutate(sample_size = n[i])
  loop_ppd <- rbind(loop_ppd, ppd_est) # save to df 
  
  # print rep
  print(i)

}

# check
head(loop_coefs3) 
tail(loop_coefs3) 
glimpse(loop_coefs3)
```

```{r}
# make a df with the true values
truth <- data.frame(intercept=a, x1=0.3, x2=1, x1x2_intxn=0.15) %>% 
   pivot_longer(intercept:x1x2_intxn, names_to = "parameter", values_to = "value")

loop_coefs3 %>% 
  rename(intercept = `(Intercept)`, 
         x1 = x1_sub, 
         x2 = x2_sub, 
         x1x2_intxn = `x1_sub:x2_sub`, 
         sample_size = V1) %>% 
  pivot_longer(intercept:x1x2_intxn, names_to = "parameter", values_to = "value") %>% 
  ggplot(aes(x=parameter, y=value, col=sample_size)) + 
  geom_jitter(width=0.15, size = 2, alpha=0.6) +
  theme_bw() +
  geom_point(aes(x=parameter, y=value), data=truth, col="black", cex=3.5) +
  scale_y_continuous(limits=c(-4, 4), breaks=seq(-4,4,1)) # set y axis to facilitate comparison across diff plots

```

**Challenge C -- Repeat step 3, but also extract an estimate of uncertainty from your posterior (e.g., 50% uncertainty intervals) and plot the results.** 
```{r, results="hide", warning=FALSE}
# note: mean_ppd was extracted above (step 3) 

head(loop_ppd)
loop_ppd %>% 
  dplyr::select(-parameter) %>% 
  pivot_longer(mean:`90%`, names_to = "type", values_to = "value") %>% 
  ggplot(aes(x=type, y=value, col=sample_size)) +
  geom_jitter(width=0.15, size=2.5, alpha=0.6) +
  theme_bw()

```


**4 -- Review the sample datasets and pick one you want to use to go through the complete workflow with (which is the homework for next week). Ideally, you'll get to work on that one, but I may ask people to pick a new one so everyone is not using the same one.**

I'd like to analyse the damselfly dataset.
